\chapter{Multigrid}


\section{Multigrid methods}

Multigrid refers to a family of methods which use multiple grid levels in order to achieve high rates of convergence to solve linear systems.
The algorithm time-complexity is linear in the number of equations, making multigrid an optimal solver for many problems.
Multigrid is applied to find the solution to elliptic PDEs, such as the Poisson problem, or our problem of focus, the Helmholtz equation.

Let us motivate multigrid by considering the downfalls of the Jacobi method.
Iteration converges rapidly for error components whose frequency components are high.
Conversely, iteration is slow for error components with low frequency components.
By transferring the residual to a coarser grid, the error components with low frequency in the fine grid have high frequency in the coarse grid.
Hence smoothing iterations converge rapidly for this new problem.
Now the solution is transferred back to the fine grid by interpolation and applying the coarse grid correction.

High frequency components of the error are damped rapidly by a smoother.
Low frequency components on a coarse mesh are high frequency, thus are damped rapidly by a smoother.

The description above is the outline for the two-grid method, on which many other multgrid methods are based \cite{hackbusch}.
For instance, the MG V-cycle may be thought of as a recursive scheme by performing an additional two-grid iteration on each coarser grid.
In this sense, we move from a fine mesh to coarser and coarser grids by restriction until we may solve the system directly.
Then we move the solution back to the finest level by prolongation until we are at the desired mesh level. 

For the finite element method to be compatible with multigrid, several components must exist.
Firstly, a hierarchy of meshes that define the multiple levels of the method, as well as transfer operators between the levels.
Next, on each level of the mesh, the Jacobian matrix resulting from the finite element discretisation must be formed in order to solve the problem at that level.
Finally, there must be  a suitable smoothing iteration procedure on each level.
In our case, this is FGMRES, however any appropriate iterative solver will be sufficient.




\subsection{Multigrid cycles}

Multigrid, as an iterative solver, involves performing cycles to reduce the residual in the problem.
The multigrid algorithm iteratively applies one of these cycles until a suitable convergence tolerance is reached.
The most common multigrid cycle is the V-cycle, and besides the two-grid method, it is the simplest to implement.
However there are many more cycles, each of which is a variation of the basic two-grid algorithm.
For example, other cycles include the W-cycle, F-cycle, and S-cycles.

Figure \ref{fig:mgcycles} shows a representation of how a single iteration of different cycles are applied.
The cycle begins on the first mesh.
Arrows pointing up indicate a prolongation to a finer mesh, and pointing down, a restriction to a coarser mesh.
On each mesh, smoothing is also performed.
A discussion of each of these components of the multigrid algorithm now follows.


\begin{figure}[h]
	\centering
	\subfloat[][\label{vcycle} V-cycle]{
	\begin{tikzpicture}[baseline]
	\def \n {2}
	\def \w {0.5}
	\def \r {0.9}
	\def \scale {0.8}
	\begin{scope}
		\foreach \x in {\n,...,1} {
		\node at (-\x*\w,\x) [circle,fill=black,scale=\scale] {};
		\draw [arrow] (-\x*\w,\x) -- (-\x*\w+\r*\w,\x-\r);
		}
		\foreach \x in {1,...,\n} {
		\node at (\x*\w,\x) [circle,fill=black,scale=\scale] {};
		}
			\foreach \x in {1,...,1} {
			\draw [arrow] (\x*\w,\x) -- (\x*\w+\r*\w, \x+\r);
		}
		\node at (0,0) [circle,fill=black,scale=\scale] {};
		\draw [arrow] (0,0) -- (\r*\w, \r);
	\end{scope}
	\end{tikzpicture}}
	\hfill
	\subfloat[][\label{wcycle} W-cycle]{
	\begin{tikzpicture}[baseline]
	\def \n {2}
	\def \w {0.5}
	\def \r {0.9}
	\def \scale {0.8}
	\begin{scope}
		\foreach \x in {\n,...,1} {
		\node at (-\x*\w,\x) [circle,fill=black,scale=\scale] {};
		\draw [arrow] (-\x*\w,\x) -- (-\x*\w+\r*\w, \x-\r);
		}
		\foreach \x in {0,...,\n} {
		\node at (2*\w + \x*\w,\x) [circle,fill=black,scale=\scale] {};
		}
			\foreach \x in {0,...,1} {
			\draw [arrow] (2*\w + \x*\w,\x) -- (2*\w + \x*\w+\r*\w, \x+\r);
		}
		\node at (0,0) [circle,fill=black,scale=\scale] {};
		\draw [arrow] (0,0) -- (\r*\w, \r);
		\node at (\w,1) [circle,fill=black,scale=\scale]{};
		\draw [arrow] (\w,1) -- (2*\w, 1-\r);
		
	\end{scope}

	\end{tikzpicture}}
	\hfill
	\subfloat[][\label{fmgcycle} F-cycle (full multigrid)]{
	\begin{tikzpicture}[baseline]
	\def \n {3}
	\def \w {0.5}
	\def \r {0.9}
	\def \scale {0.8}
	\begin{scope}
		
		\node at (0,0) [circle,fill=black,scale=\scale] {};
		\node at (\w,1) [circle,fill=black,scale=\scale] {};
		\node at (2*\w,0) [circle,fill=black,scale=\scale] {};
		\node at (3*\w,1) [circle,fill=black,scale=\scale] {};
		\node at (4*\w,2) [circle,fill=black,scale=\scale] {};
		\node at (5*\w,1) [circle,fill=black,scale=\scale] {};
		\node at (6*\w,0) [circle,fill=black,scale=\scale] {};
		\node at (7*\w,1) [circle,fill=black,scale=\scale] {};
		\node at (8*\w,2) [circle,fill=black,scale=\scale] {};
		
		\draw [arrow] (0,0) -- (\w, \r);
		\draw [arrow] (\w,1) -- (2*\w, 1-\r);
		\draw [arrow] (2*\w,0) -- (3*\w, \r);
		\draw [arrow] (3*\w,1) -- (4*\w, 1+\r);
		\draw [arrow] (4*\w, 2) -- (5*\w, 2-\r);
		\draw [arrow] (5*\w, 1) -- (6*\w, 1-\r);
		\draw [arrow] (6*\w, 0) -- (7*\w, \r);
		\draw [arrow] (7*\w, 1) -- (8*\w, 1+\r);
	\end{scope}
	\end{tikzpicture}}
	\caption{\label{fig:mgcycles} Examples of multigrid cycles on a sequence of meshes with three levels.}
\end{figure}







%------------------------------------------------

\section{Components of a multigrid algorithm}

\subsection{Smoothing}

Smoothing is not used to solve the problem, although on its own the smoother may be used as a convergent iterative solver.
Instead, the purpose is to reduce the high frequency errors of the solution on the current grid level.
As mentioned earlier, this is the key idea to multigrid algorithms.
Smoothing is typically done by a SOR method, such as the weighted Jacobi or Gauss-Seidel methods.

\begin{figure}
	% Insert figure of error components for initial iteration phases, want to show low frequency errors are not dampened.
	% Insert figure for how the frequencies decay based on mesh level.
	% Probably use a 1D Helmholtz problem to do this
	\centering
	\includegraphics[draft]{images/placeholder}
	\caption{High frequency errors are damped fast}
\end{figure}


There are two opportunities to perform smoothing operations.
These occur before the restriction to the coarser level (pre-smoothing), then again after the coarse grid correction is applied to the solution on the finer level (post-smoothing).
Different multigrid cycles apply different rules to smoothing, such as when to smooth and how many smoothing iterations to perform.
For example, in the case of a V or W-cycle, both pre-smoothing and post-smoothing are performed.
However, in the case of an S-cycle, no pre-smoothing operations are performed, and only post-smoothing is applied.
For certain problems, this can improve performance because of the reduction of the smoothing operations \cite{iyengar}.


\cite{elman}
The particular choice of smoother in \oomph is between the complex damped Jacobi method and GMRES.
Alternatively, one could choose to implement their own smoother to override the default behaviour.
Generally speaking, 

In this paper, the results and analysis are obtained from using the GMRES solver.




\subsection{Prolongation and restriction}

The procedures for transferring solution data between mesh levels are known as prolongation and restriction.
These operators are built 

Prolongation, otherwise known as interpolation, is the process of transferring data up a level from a coarse grid to a finer grid.
This is done in general by sampling known values to interpolate the unknowns.

Let $\Omega^h$ and $\Omega^{2h}$ be the meshes on the fine and coarse levels, respectively, and let $V^{h}$ and $V^{2h}$ be the finite element spaces on their respective meshes.
Since $\Omega^{h}$ is obtained through refinement of $\Omega^{2h}$, we have that $V^{2h}\subset V^h$.
Then we can define the canonical prolongation operator \cite{volker},
\begin{align}
	I^h_{2h} \, : \, V^{2h} \rightarrow V^h.
\end{align}

As an alternative view of the transfer operator above, we can consider the operator acting on nodal values instead of acting on the members of the finite element space.
Using this approach, if the coarse mesh with a grid resolution $2h$ has $n$ total nodes, and the fine mesh with a grid resolution of $h$ has $m$ total nodes, then
\begin{align}
	I^h_{2h} \, : \, R^m \rightarrow R^n.
\end{align}



Restriction, sometimes known as coarsening or injection, is the `inverse' operation to prolongation.
That is, in the sense that restriction operations perform the opposite action to prolongation.
This process transfers data down a grid level from a fine grid to a coarser grid.

Using the notation as above, we can define the canonical restriction operators \cite{volker},
\begin{align}
	I^{2h}_h \, : \, V^{h} \rightarrow V^{2h}.
\end{align}



On any given mesh level, applying first a restriction then a prolongation, or a prolongation then a restriction, will result in the mesh level remaining constant.
However, it is not necessarily true that the solution data will remain the same.
Since the prolongation operator interpolates data values, certainly there will be some discrepancy in the final result.

Figure \ref{fig:pro_res_ops} is a representation of how the transfer matrices act on points in the mesh.


\begin{figure}[h]
	\centering
	\scalebox{0.4}{
	\begin{tikzpicture}[baseline,decoration={markings,mark=at position 0.08 with
		{\arrow[scale=5,>=stealth]{<}}}]
	\centering
	\begin{scope}
	
	% define constants
	\def \w {8}
	\def \d {4}
	\def \textoff {3}
	\def \r {6}
	
	% Draw the grids
	\draw[step=2,black,very thin] (0,0) grid (\w,\w);
	\draw[step=4,black,very thin] (\w+\d,0) grid (2*\w+\d,8);
	\draw[step=8,black,very thin] (2*\w+2*\d,0) grid (3*\w+2*\d,8);
	
	% First text
	\node[anchor=center,scale=2.5] at (\w+\d/2,\w+\textoff) {Restriction};
	\node[anchor=center,scale=2.5] at (\w+\d/2.1,-\textoff) {Prolongation};
	
	% Second text
	\node[anchor=center,scale=2.5] at (2*\w+1.5*\d,\w+\textoff) {Restriction};
	\node[anchor=center,scale=2.5] at (2*\w+1.5*\d,-\textoff) {Prolongation};
	
	% Top arrows
	\draw[postaction={decorate}] (\w+\d+\r/4,\w+0.5) arc (55:125:\r);
	\draw[postaction={decorate}] (2*\w+2*\d+\r/4,\w+0.5) arc (55:125:\r);
	
	% Bottom arrows
	\draw[postaction={decorate}] (\w-\r/4,-0.5) arc (180+55:180+125:\r);
	\draw[postaction={decorate}] (2*\w+\d-\r/4,-0.5) arc (180+55:180+125:\r);
	
	\end{scope}
	\end{tikzpicture}
	}
	
	\caption{\label{fig:pro_res_ops} Prolongation and restriction operators moving between mesh levels.}
\end{figure}



Two transfer operators are required for each mesh level, except for on the finest and coarsest levels.
Instead, a restriction for the coarsest level and a prolongation the matrix is not needed.
All of these are created in the setup phase of the algorithm.
The algorithm used in the \oomph implementation of operator creation process is described below.

To move from a coarse mesh to a fine mesh:
1. Create the fine mesh
2. For each element in the fine mesh
3. For each node in the element
4. Get the local coordinates of the node
5. Interpolate the value of the solution using the coarse mesh solution value and basis functions
6. End loops

Once the prolongation matrix has been created, the restriction matrix is then defined to be the transpose of the prolongation matrix.
Figure \ref{fig:create_prolong_matrix} shows how solution data for a uniform refinement of a single element is performed.

\begin{figure}[h]
	\centering
	\includegraphics[draft]{images/placeholder}
	\caption{Prolongation matrix creation process. \label{fig:create_prolong_matrix}}
\end{figure}




\subsection{Grid hierarchy}

We will restrict our discussion to uniform structured grids in order to focus on the details of multigrid.
Mathematically, we require only the grid hierarchy so the specific internals of the individual grids is a technical detail that we will omit. 

% We want to discuss how the grid hierarchy is computed and used within oomph-lib.
The grid hierarchy used by multigrid in \oomph is generated by... 



\begin{figure}[h]
	% Show initial refinement, want to show how the hierarchy is created within oomph-lib
	\centering
	\includegraphics[draft]{images/placeholder}
	\caption{Example of a grid hierarchy created for multigrid in \oomph.}
\end{figure}





%------------------------------------------------

\section{Multigrid as a preconditioner}
\label{sec:precond}

\subsection{Preconditioning}

% General talk about preconditioning
We want to solve the linear system
\begin{align}
	A x = b.
\end{align}
This equation is generated from the finite element discretisation of the problem.
Linear systems of equations are everywhere, so there are a choice of several different methods to use to solve.
Direct methods are too costly to use since in general, the dimension of the problem is large.
Iterative methods, such as Krylov subspace methods, are a good alternative to use.
However the performance of such methods is poor without a preconditioner.
Hence, we would like a preconditioner that improves the performance of the iterative method.

Applying a preconditioning matrix $P$ on the left, we have
\begin{align}
	P^{-1} A x = P^{-1} b, \label{eqn:precond_left}
\end{align}
which solves the above system exactly for the case $P=A^{-1}$. 
This is optimal as it solves the original problem, however it is costly to obtain.
Instead, we would like a alternative such that $P$ is cheap to find and the preconditioned system \eqref{eqn:precond_left} is easier to solve than the original problem.

Alternatively, a preconditioning matrix can be applied on the right.
In this case, we have
\begin{align}
	A P^{-1} P x = b. \label{eqn:precond_right}
\end{align}
This can be solved in two steps, 
\begin{align}
	P x = y, \qquad \qquad A P^{-1} y = b.
\end{align}


Many general preconditioning techniques have been developed that work for a wide variety of problems.
For example, preconditioning matrices based on the splitting of the matrix $A$ are simple to use and cheap to compute.
More specific preconditioning techniques have been developed in the case of certain problems.
These tend to be more complicated to use, but often will come with better performance.
As a first attempt, it is a good idea to use a simple method to see the impact on the convergence times.
Only in the case where optimisation is crucial to the problem or a general preconditioner does not work, a specific preconditioner should be developed.



\subsection{Block preconditioning}

% Talk about block preconditioning in regards to complex Helmholtz problem
% The block structure, derivation...
% Enumeration of the unknowns and equations. Real part first, imaginary part second

As mentioned in chapter \ref{sec:problem}, the Helmholtz equation we are interested in is complex-valued.
Since \oomph does not natively handle complex arithmetic, complex numbers are instead stored as 2D vectors.
The first component of the vector is the real part of the complex number, and the second component is the imaginary part.

For a specific enumeration of the unknowns, we obtain a matrix whose structure allows us to solve the problem more efficiently.
In our case, we first compute the contribution for all of the real unknowns, then the contribution for the imaginary unknowns.
This gives rise to a Jacobian matrix with block structure
\begin{align}
	A = \begin{pmatrix}
		A_r & -A_c \\ A_c & A_r
	\end{pmatrix}.
\end{align}
Here, $A_r$ is the real part of the discretisation and $A_c$ is the imaginary part.

Given this matrix with a block structure, we will want to use a block preconditioner as our preconditioning matrix.




\subsection{Multigrid preconditioning}

As an alternative to using multigrid as an iterative solver, it is possible to instead perform multigrid iterations to partially solve the linear system, then perform a solve with an additional linear solver on the preconditioned system.
This gives an effective method  

On its own, the linear solver is not as effective as the preconditioned linear solver.


We reduce the tolerance of the mg solver and 





\subsection{FGMRES}

Multigrid preconditioning leads to a variable preconditioner.
That is, between iterations, the preconditioning matrix changes.

Because of this, the standard GMRES method is not a suitable choice of smoother.
Instead, the flexible version of the GMRES algorithm, appropriately named FGMRES, may be used instead.

Only capable of preconditioning on the right


This method was proposed by Saad in \cite{fgmres}.
For a more detailed description of the FGMRES algorithm, readers are directed to this paper.






%------------------------------------------------

\section{Variable coefficients}

A common issue with multigrid occurs with variable coefficient problems.
\cite{briggs}
% To demonstrate this, 
% Consider the one dimensional equation
% \begin{align}
% 	\left( a(x)u'(x) \right)' = f(x)
% \end{align}
% This is generally an issue when $a(x)$ is highly variable.

In Cartesian coordinates, the Helmholtz equation is a constant coefficient problem.
The operator we have in this case is
\begin{align}
	\nabla^2 + k^2 = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial t^2} + \frac{\partial^2}{\partial z^2} + k^2.
\end{align}
Taking $k=0$, we obtain the Laplace operator.
This is especially well behaved for multigrid methods.
Aside from the issues mentioned above, particularly with matrices becoming indefinite for larger values of $k^2$, this equation has constant coefficients.
Therefore, combining the preconditioning discussed in section \ref{sec:precond}, multigrid should converge appropriately.
Indeed, the multigrid implementation within \oomph for the Cartesian solver behaves as expected.

In cylindrical coordinates, however, we have a term that varies proportional to the inverse of the radial distance squared.
For the full Fourier decomposed Helmholtz equation, recall equation \eqref{eqn:fhh},
\begin{align}
	\frac{\partial^2 u_N}{\partial r^2}
			 + \frac{1}{r} \frac{\partial u_N}{\partial r}
			 + \frac{\partial^2 u_N}{\partial z^2}
			 + (k^2 - \frac{N^2}{r^2})u_N = 0. \label{eqn:full_fhh}
\end{align}

Before any theoretical work to attempt to remedy issues arising from variable coefficients, we should first be certain that a problem exists with multigrid applied to the cylindrical problem.
Since the implementation of multigrid within \oomph exists for the Cartesian case, we can begin here.
With some slight modifications, the same code can be used to solve the problem in cylinderical coordinates.
% Mention the changes made?
Running the modified code to compute a solution resulted in a failure.
The general error was lack of convergence in the FGMRES solver.
To deduce the precide nature of the error, more analysis of the problem is necessary.




\subsection{A model problem}

% How do we overcome this issue?
% 	Implement variable mesh size
% 	Move the domain so that the coefficients are not rapidly varying
% 	1/r^2 varies close to the origin

We present a simplification of the variable coefficient problem to identify issues with the method.
Consider the one dimensional, variable coefficient problem on the domain $\left[a,b\right]$,
\begin{align}
	u''(r) + \frac{1}{r}u'(r) = f(r), \label{eqn:model}
\end{align}
with dirichlet boundary conditions given by
\begin{align}
	u(a)=\alpha, \qquad \qquad u(b)=\beta.
\end{align}
This equation represents the first two terms, the radial part, in \eqref{eqn:full_fhh}.
The coefficient of $u'(r)$ varies by the inverse of the radial distance from the origin.

The simplifications made in this equation
Reducing the dimension to one, dropping all but two terms

Why can we say that this is ok?
The two equations both have a singularity at $r=0$.
Both have varying coefficients.
Same order equation.


The solution to the homogeneous equation is 
\begin{align}
	u_h(r) = A \log(r) + B.
\end{align}
The solution to the full nonhomogeneous equation can be found using your favourite method.

$1/r$ changes rapidly close to the origin, and as we move away from 0, the change becomes less signficicant.

% We use a finite difference scheme because?
To solve this model problem, we discretise the equation using a finite difference scheme.
First, we discretise the domain to generate a mesh with $N+2$ points, letting $j=0,1,\ldots,N+1$ index the mesh.
\begin{align}
	h &= \frac{1}{N+1}, \qquad u_0 = \alpha, \qquad u_{N+1} = \beta \\
	x_j &= a + jh, \qquad j=1,2,\ldots,N \\
	u_j &= u(x_j), \qquad j=1,2,\ldots,N \\
	f_j &= f(x_j), \qquad j=1,2,\ldots,N
\end{align}

Next, we discretise the equation.
We use a second order centered difference for the second order term and a first order forward difference for the first order term.
This leads to the stencil
\begin{align}
	\frac{u_{j+1} - 2u_{j} + u_{j-1}}{h^2} + \frac{1}{jh}\frac{u_{j+1}-u_{j}}{h} = f_j, \qquad j=1,2,\ldots,N.
\end{align}


We then extend a previous implementation of a multigrid solver, adapted from the code developed in \cite{weir}.

To compare to a baseline, we also solve the constant coefficient problem,
\begin{align}
	u''(r) = f(r).
\end{align}
This is the one dimensional Poisson equation, and as mentioned above is solved well by multigrid.

The results in table \ref{tab:model} show...

\begin{table}[h]
	\centering
	\begingroup
	\renewcommand*{\arraystretch}{1.5}
	\begin{tabular}{c|c|c}
		col1 & col2 & col3 \\ \hline
		col1 & col2 & col3 \\
		col1 & col2 & col3 \\
	\end{tabular}
	\endgroup
	\caption{Results for model problem. \label{tab:model}}
\end{table}

Figure \ref{fig:model} below...

\begin{figure}[h]
	% Show the iteration count increasing
	\centering
	\subfloat[Iteration counts.]{\includegraphics[draft]{images/placeholder}}

	% Residuals and errors
	% \includegraphics[draft]{images/placeholder}
	\subfloat[Residuals and absolute and relative errors.]{\includegraphics[draft]{images/placeholder}}
	\caption{Convergence results for multigrid applied to equation \eqref{eqn:model}. \label{fig:model}}
\end{figure}





%------------------------------------------------

\section{Analysis of multigrid}

% Where do the eigenvalues lie?
% How does the convergence behave?
% We want to reference \cite{cslp2}


