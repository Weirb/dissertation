\chapter{Multigrid}


\section{Multigrid methods}

Multigrid refers to a family of methods which use multiple grid levels in order to achieve high rates of convergence to solve linear systems.
The algorithm time-complexity is linear in the number of equations, making multigrid an optimal solver for many problems.
Multigrid is applied to find the solution to elliptic PDEs, such as the Poisson problem, or our problem of focus, the Helmholtz equation.

Let us motivate multigrid by considering the downfalls of the Jacobi method.
Iteration converges rapidly for error components whose frequency components are high.
Conversely, iteration is slow for error components with low frequency components.
By transferring the residual to a coarser grid, the error components with low frequency in the fine grid have high frequency in the coarse grid.
Hence smoothing iterations converge rapidly for this new problem.
Now the solution is transferred back to the fine grid by interpolation and applying the coarse grid correction.

High frequency components of the error are damped rapidly by a smoother.
Low frequency components on a coarse mesh are high frequency, thus are damped rapidly by a smoother.

The description above is the outline for the two-grid method, on which many other multgrid methods are based \cite{hackbusch}.
For instance, the MG V-cycle may be thought of as a recursive scheme by performing an additional two-grid iteration on each coarser grid.
In this sense, we move from a fine mesh to coarser and coarser grids by restriction until we may solve the system directly.
Then we move the solution back to the finest level by prolongation until we are at the desired mesh level. 

For the finite element method to be compatible with multigrid, several components must exist.
Firstly, a hierarchy of meshes that define the multiple levels of the method, as well as transfer operators between the levels.
Next, on each level of the mesh, the Jacobian matrix resulting from the finite element discretisation must be formed in order to solve the problem at that level.
Finally, there must be  a suitable smoothing iteration procedure on each level.
In our case, this is FGMRES, however any appropriate iterative solver will be sufficient.




\subsection{Multigrid cycles}

Multigrid, as an iterative solver

The most common cycles are the V and W-cycles, however there are many more variations of the basic two-grid algorithm, among which are the F and S-cycles.
Notably, beyond the two-grid method, the V-cycle is the simplest in its symmetry.

The multigrid algorithm consists of iteratively applying one of these cycles until a suitable convergence tolerance is reached.
As mentioned above, the 



\begin{figure}
	% Insert figures of the grid hierarchy for 2-grid, V-cycle, etc.
	% This figure uses dots, same as undergraduate figure
	\centering
	\includegraphics[draft]{images/placeholder}
	\caption{V-cycle, W-cycle, F-cycle, S-cycle}
\end{figure}







%------------------------------------------------

\section{Components of a multigrid algorithm}

\subsection{Smoothing}

Smoothing is not used to solve the problem, although on its own the smoother may be used as a convergent iterative solver.
Instead, the purpose is to reduce the high frequency errors of the solution on the current grid level.
As mentioned earlier, this is the key idea to multigrid algorithms.
Smoothing is typically done by a SOR method, such as the weighted Jacobi or Gauss-Seidel methods.

\begin{figure}
	% Insert figure of error components for initial iteration phases, want to show low frequency errors are not dampened.
	% Insert figure for how the frequencies decay based on mesh level.
	% Probably use a 1D Helmholtz problem to do this
	\centering
	\includegraphics[draft]{images/placeholder}
	\caption{High frequency errors are damped fast}
\end{figure}


There are two opportunities to perform smoothing operations.
These occur before the restriction to the coarser level (pre-smoothing), then again after the coarse grid correction is applied to the solution on the finer level (post-smoothing).
Different multigrid cycles apply different rules to smoothing, such as when to smooth and how many smoothing iterations to perform.
For example, in the case of a V or W-cycle, both pre-smoothing and post-smoothing are performed.
However, in the case of an S-cycle, no pre-smoothing operations are performed, and only post-smoothing is applied.
For certain problems, this can improve performance because of the reduction of the smoothing operations \cite{iyengar}.

The particular choice of smoother in \oomph is between the complex damped Jacobi method and GMRES.
Alternatively, one could choose to implement their own smoother to override the default behaviour.
Generally speaking, 

In this paper, the results and analysis are obtained from using the GMRES solver.


\subsubsection{FGMRES}

In general, Krylov subspace methods should not be used for smoothing operations.
This is because of their property that...

Hence, the standard GMRES method is not a suitable choice of smoother.
Because of this, the flexible version of the GMRES algorithm, appropriately named FGMRES, may be used instead.
This method was proposed by Saad \cite{fgmres}, 





\subsection{Prolongation and restriction}

Prolongation, sometimes known as interpolation, is the process of transferring data up a level from a coarse grid to a finer grid.

Let $\Omega^h$ and $\Omega^{2h}$ be the meshes on the fine and coarse levels, respectively, and let $V^{h}$ and $V^{2h}$ be the finite element spaces on their respective meshes.
Since $\Omega^{h}$ is obtained through refinement of $\Omega^{2h}$, we have that $V^{2h}\subset V^h$.
Then we can define the canonical prolongation operator \cite{volker},
\begin{align}
	I^h_{2h} \, : \, V^{2h} \rightarrow V^h.
\end{align}


This operator can often be represented by a matrix in 

The simplest method of prolongation is an averaging procedure, which linearly interpolates the values in

This is done by sampling

% How does oomph-lib do it?



Restriction, sometimes known as coarsening or injection, is the `inverse' operation to prolongation.
This process transfers data down a grid level from a fine grid to a coarser grid.

The simplest restriction operation is the injection, which 


As above, let 


% Transpose of prolongation
% Many choices, no need to use inverse of prolongation
% Also called coarsening



\begin{figure}
	% Show the process of moving up/down the levels, using dots again
	\centering
	\includegraphics[draft]{images/placeholder}
	\caption{Prolongation and restriction operators acting on sample points}
\end{figure}




\subsection{Grid hierarchy}

We will restrict our discussion to uniform structured grids in order to focus on the details of multigrid.
Mathematically, we require only the grid hierarchy so the specific internals of the individual grids is a technical detail that we will omit. 

% We want to discuss how the grid hierarchy is computed and used within oomph-lib.
The grid hierarchy used by multigrid in \oomph is generated by... 



\begin{figure}
	% Show initial refinement, want to show how the hierarchy is created within oomph-lib
	\centering
	\includegraphics[draft]{images/placeholder}
	\caption{Example of a grid hierarchy created for multigrid in \oomph.}
\end{figure}





%------------------------------------------------

\section{Multigrid as a preconditioner}
\label{sec:precond}

\subsection{Preconditioning}

% General talk about preconditioning
We want to solve the linear system
\begin{align}
	A x = b.
\end{align}
Direct methods are too costly to use since in general, the dimension of the problem is large.
Iterative methods, such as Krylov subspace methods, are a good alternative to use.
However the performance of such methods is poor without a preconditioner.
Hence, we would like a method that improves the performance of the iterative method.


Applying a preconditioning matrix $P$ on the left, we have
\begin{align}
	P^{-1} A x = P^{-1} b \label{eqn:precond}
\end{align}
which solves the above system exactly for the case $P=A^{-1}$. 
This is optimal as it solves the original problem, however it is costly to obtain.
Instead, we would like a alternative such that $P$ is cheap to find and the preconditioned system \eqref{eqn:precond} is easier to solve than the original problem.


Many general preconditioning techniques have been developed that work for a wide variety of problems.
For example, preconditioning matrices based on the splitting of the matrix $A$ are simple to use and cheap to compute.
More specific preconditioning techniques have been developed in the case of certain problems.
These tend to be more complicated to use, but often will come with better performance.
As a first attempt, it is a good idea to use a simple method to see the impact on the convergence times.
Only in the case where optimisation is crucial to the problem or a general preconditioner does not work, a specific preconditioner should be developed.



\subsection{Block preconditioning}

% Talk about block preconditioning in regards to complex Helmholtz problem
% The block structure, derivation...


\subsection{Multigrid preconditioning}

As an alternative to using multigrid as an iterative solver, it is possible to instead perform multigrid iterations to partially solve the linear system, then perform a solve with an additional linear solver on the preconditioned system.
This gives an effective method  

On its own, the linear solver is not as effective as the preconditioned linear solver.



We reduce the tolerance of the mg solver and 




%------------------------------------------------

\section{Variable coefficients}

A common issue with multigrid occurs with variable coefficient problems.
\cite{briggs}
% To demonstrate this, 
% Consider the one dimensional equation
% \begin{align}
% 	\left( a(x)u'(x) \right)' = f(x)
% \end{align}
% This is generally an issue when $a(x)$ is highly variable.

In Cartesian coordinates, the Helmholtz equation is a constant coefficient problem.
The operator we have in this case is
\begin{align}
	\nabla^2 + k^2 = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial t^2} + \frac{\partial^2}{\partial z^2} + k^2.
\end{align}
Taking $k=0$, we obtain the Laplace operator.
This is especially well behaved for multigrid methods.
Aside from the issues mentioned above, particularly with matrices becoming indefinite for larger values of $k^2$, this equation has constant coefficients.
Therefore, combining the preconditioning discussed in section \ref{sec:precond}, multigrid should converge appropriately.
Indeed, the multigrid implementation within \oomph for the Cartesian solver behaves as expected.

In cylindrical coordinates, however, we have a term that varies proportional to the inverse of the radial distance squared.
For the full Fourier decomposed Helmholtz equation, recall equation \eqref{eqn:fhh},
\begin{align}
	\frac{\partial^2 u_N}{\partial r^2}
			 + \frac{1}{r} \frac{\partial u_N}{\partial r}
			 + \frac{\partial^2 u_N}{\partial z^2}
			 + (k^2 - \frac{N^2}{r^2}) = 0. \label{eqn:full_fhh}
\end{align}

Before any theoretical work to attempt to remedy issues arising from variable coefficients, we should first be certain that a problem exists with multigrid applied to the cylindrical problem.

Computing a solution using multigrid
The analysis of 



\subsection{A model problem}

% How do we overcome this issue?
% 	Implement variable mesh size
% 	Move the domain so that the coefficients are not rapidly varying
% 	1/r^2 varies close to the origin

We present a simplification of the variable coefficient problem to identify issues with the method.
Consider the one dimensional, variable coefficient problem,
\begin{align}
	u''(r) + \frac{1}{r}u'(r) = f(r). \label{eqn:model}
\end{align}
This equation represents the first two terms in \eqref{eqn:full_fhh}.
The coefficient of $u'(r)$ varies by the inverse of radial distance.

The simplifications made in this equation
Reducing the dimension to one, dropping all but two terms

Why can we say that this is ok?
The two equations both have a singularity at $r=0$.
Both have varying coefficients.
Same order equation.


The solution to the homogeneous equation is 
\begin{align}
	u_h(r) = A \log(r) + B
\end{align}

$1/r$ changes rapidly close to the origin, and as we move away from 0, the change becomes less signficicant.


To solve this model problem, the equation was discretised using a finite difference scheme.
% We use a finite difference scheme because 
The stencil for solving this equation is given by
\begin{align}
	% stencil
\end{align}
We then implement a multigrid solver in Python, adapted from the code developed in \cite{weir}.

To compare to a baseline, we also solve the constant coefficient problem,
\begin{align}
	u''(r) = f(r).
\end{align}
This is the one dimensional Poisson equation, and as mentioned above is solved well by multigrid.

The results show...


\begin{figure}[h]
	% Show how multigrid behaves for the above equation.
	% Need to decide on 
	\centering
	\includegraphics[draft]{images/placeholder}
	\caption{Convergence analysis of multigrid applied to equation \eqref{eqn:model}}
\end{figure}






%------------------------------------------------

\section{Analysis of multigrid}

% Where do the eigenvalues lie?
% How does the convergence behave?
% We want to reference \cite{cslp2}


