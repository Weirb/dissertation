\chapter{Multigrid}


\section{Multigrid methods}

Multigrid refers to a family of methods which use multiple grid levels in order to achieve high rates of convergence to solve linear systems.
The algorithm time-complexity is linear in the number of equations, making multigrid an optimal solver for many problems.
Multigrid is applied to find the solution to elliptic PDEs, such as the Poisson problem, or our problem of focus, the Helmholtz equation.

Let us motivate multigrid by considering the downfalls of the Jacobi method.
Iteration converges rapidly for error components whose frequency components are high.
Conversely, iteration is slow for error components with low frequency components.
By transferring the residual to a coarser grid, the error components with low frequency in the fine grid have high frequency in the coarse grid.
Hence smoothing iterations converge rapidly for this new problem.
Now the solution is transferred back to the fine grid by interpolation and applying the coarse grid correction.

High frequency components of the error are damped rapidly by a smoother.
Low frequency components on a coarse mesh are high frequency, thus are damped rapidly by a smoother.

The description above is the outline for the two-grid method, on which many other multgrid methods are based \cite{hackbusch}.
For instance, the MG V-cycle may be thought of as a recursive scheme by performing an additional two-grid iteration on each coarser grid.
In this sense, we move from a fine mesh to coarser and coarser grids by restriction until we may solve the system directly.
Then we move the solution back to the finest level by prolongation until we are at the desired mesh level. 

For the finite element method to be compatible with multigrid, several components must exist.
Firstly, a hierarchy of meshes that define the multiple levels of the method, as well as transfer operators between the levels.
Next, on each level of the mesh, the Jacobian matrix resulting from the finite element discretisation must be formed in order to solve the problem at that level.
Finally, there must be  a suitable smoothing iteration procedure on each level.
In our case, this is FGMRES, however any appropriate iterative solver will be sufficient.




\subsection{Multigrid cycles}

Multigrid, as an iterative solver, involves performing cycles to reduce the residual in the problem.
The multigrid algorithm iteratively applies one of these cycles until a suitable convergence tolerance is reached.
The most common multigrid cycle is the V-cycle, and besides the two-grid method, it is the simplest to implement.
However there are many more cycles, each of which is a variation of the basic two-grid algorithm.
For example, other cycles include the W-cycle, F-cycle, and S-cycles.

Figure \ref{fig:mgcycles} shows a representation of how a single iteration of different cycles are applied.
The cycle begins on the first mesh.
Arrows pointing up indicate a prolongation to a finer mesh, and pointing down, a restriction to a coarser mesh.
On each mesh, smoothing is also performed.
A discussion of each of these components of the multigrid algorithm now follows.


\begin{figure}[h]
	\centering
	\subfloat[][\label{vcycle} V-cycle]{
	\begin{tikzpicture}[baseline]
	\def \n {2}
	\def \w {0.5}
	\def \r {0.9}
	\def \scale {0.8}
	\begin{scope}
		\foreach \x in {\n,...,1} {
		\node at (-\x*\w,\x) [circle,fill=black,scale=\scale] {};
		\draw [arrow] (-\x*\w,\x) -- (-\x*\w+\r*\w,\x-\r);
		}
		\foreach \x in {1,...,\n} {
		\node at (\x*\w,\x) [circle,fill=black,scale=\scale] {};
		}
			\foreach \x in {1,...,1} {
			\draw [arrow] (\x*\w,\x) -- (\x*\w+\r*\w, \x+\r);
		}
		\node at (0,0) [circle,fill=black,scale=\scale] {};
		\draw [arrow] (0,0) -- (\r*\w, \r);
	\end{scope}
	\end{tikzpicture}}
	\hfill
	\subfloat[][\label{wcycle} W-cycle]{
	\begin{tikzpicture}[baseline]
	\def \n {2}
	\def \w {0.5}
	\def \r {0.9}
	\def \scale {0.8}
	\begin{scope}
		\foreach \x in {\n,...,1} {
		\node at (-\x*\w,\x) [circle,fill=black,scale=\scale] {};
		\draw [arrow] (-\x*\w,\x) -- (-\x*\w+\r*\w, \x-\r);
		}
		\foreach \x in {0,...,\n} {
		\node at (2*\w + \x*\w,\x) [circle,fill=black,scale=\scale] {};
		}
			\foreach \x in {0,...,1} {
			\draw [arrow] (2*\w + \x*\w,\x) -- (2*\w + \x*\w+\r*\w, \x+\r);
		}
		\node at (0,0) [circle,fill=black,scale=\scale] {};
		\draw [arrow] (0,0) -- (\r*\w, \r);
		\node at (\w,1) [circle,fill=black,scale=\scale]{};
		\draw [arrow] (\w,1) -- (2*\w, 1-\r);
		
	\end{scope}

	\end{tikzpicture}}
	\hfill
	\subfloat[][\label{fmgcycle} F-cycle (full multigrid)]{
	\begin{tikzpicture}[baseline]
	\def \n {3}
	\def \w {0.5}
	\def \r {0.9}
	\def \scale {0.8}
	\begin{scope}
		
		\node at (0,0) [circle,fill=black,scale=\scale] {};
		\node at (\w,1) [circle,fill=black,scale=\scale] {};
		\node at (2*\w,0) [circle,fill=black,scale=\scale] {};
		\node at (3*\w,1) [circle,fill=black,scale=\scale] {};
		\node at (4*\w,2) [circle,fill=black,scale=\scale] {};
		\node at (5*\w,1) [circle,fill=black,scale=\scale] {};
		\node at (6*\w,0) [circle,fill=black,scale=\scale] {};
		\node at (7*\w,1) [circle,fill=black,scale=\scale] {};
		\node at (8*\w,2) [circle,fill=black,scale=\scale] {};
		
		\draw [arrow] (0,0) -- (\w, \r);
		\draw [arrow] (\w,1) -- (2*\w, 1-\r);
		\draw [arrow] (2*\w,0) -- (3*\w, \r);
		\draw [arrow] (3*\w,1) -- (4*\w, 1+\r);
		\draw [arrow] (4*\w, 2) -- (5*\w, 2-\r);
		\draw [arrow] (5*\w, 1) -- (6*\w, 1-\r);
		\draw [arrow] (6*\w, 0) -- (7*\w, \r);
		\draw [arrow] (7*\w, 1) -- (8*\w, 1+\r);
	\end{scope}
	\end{tikzpicture}}
	\caption{\label{fig:mgcycles} Examples of multigrid cycles on a sequence of meshes with three levels.}
\end{figure}







%------------------------------------------------

\section{Components of a multigrid algorithm}

\subsection{Smoothing}

Smoothing is not used to solve the problem, although on its own the smoother may be used as a convergent iterative solver.
Instead, the purpose is to reduce the high frequency errors of the solution on the current grid level.
As mentioned earlier, this is the key idea to multigrid algorithms.
Smoothing is typically done by a SOR method, such as the weighted Jacobi or Gauss-Seidel methods.

\begin{figure}
	% Insert figure of error components for initial iteration phases, want to show low frequency errors are not dampened.
	% Insert figure for how the frequencies decay based on mesh level.
	% Probably use a 1D Helmholtz problem to do this
	\centering
	\includegraphics[draft]{images/placeholder}
	\caption{High frequency errors are damped fast}
\end{figure}


There are two opportunities to perform smoothing operations.
These occur before the restriction to the coarser level (pre-smoothing), then again after the coarse grid correction is applied to the solution on the finer level (post-smoothing).
Different multigrid cycles apply different rules to smoothing, such as when to smooth and how many smoothing iterations to perform.
For example, in the case of a V or W-cycle, both pre-smoothing and post-smoothing are performed.
However, in the case of an S-cycle, no pre-smoothing operations are performed, and only post-smoothing is applied.
For certain problems, this can improve performance because of the reduction of the smoothing operations \cite{iyengar}.


\cite{elman}
The particular choice of smoother in \oomph is between the complex damped Jacobi method and GMRES.
Alternatively, one could choose to implement their own smoother to override the default behaviour.
Generally speaking, 

In this paper, the results and analysis are obtained from using the GMRES solver.




\subsection{Prolongation and restriction}

The procedures for transferring solution data between mesh levels are known as prolongation and restriction.
These operators are built 

Prolongation, otherwise known as interpolation, is the process of transferring data up a level from a coarse grid to a finer grid.
This is done in general by sampling known values to interpolate the unknowns.

Let $\Omega^h$ and $\Omega^{2h}$ be the meshes on the fine and coarse levels, respectively, and let $V^{h}$ and $V^{2h}$ be the finite element spaces on their respective meshes.
Since $\Omega^{h}$ is obtained through refinement of $\Omega^{2h}$, we have that $V^{2h}\subset V^h$.
Then we can define the canonical prolongation operator \cite{volker},
\begin{align}
	I^h_{2h} \, : \, V^{2h} \rightarrow V^h.
\end{align}

As an alternative view of the transfer operator above, we can consider the operator acting on nodal values instead of acting on the members of the finite element space.
Using this approach, if the coarse mesh with a grid resolution $2h$ has $n$ total nodes, and the fine mesh with a grid resolution of $h$ has $m$ total nodes, then
\begin{align}
	I^h_{2h} \, : \, R^m \rightarrow R^n.
\end{align}



Restriction, sometimes known as coarsening or injection, is the `inverse' operation to prolongation.
That is, in the sense that restriction operations perform the opposite action to prolongation.
This process transfers data down a grid level from a fine grid to a coarser grid.

Using the notation as above, we can define the canonical restriction operators \cite{volker},
\begin{align}
	I^{2h}_h \, : \, V^{h} \rightarrow V^{2h}.
\end{align}



On any given mesh level, applying first a restriction then a prolongation, or a prolongation then a restriction, will result in the mesh level remaining constant.
However, it is not necessarily true that the solution data will remain the same.
Since the prolongation operator interpolates data values, certainly there will be some discrepancy in the final result.

Figure \ref{fig:pro_res_ops} is a representation of how the transfer matrices act on points in the mesh.


\begin{figure}[h]
	\centering
	\scalebox{0.4}{
	\begin{tikzpicture}[baseline,decoration={markings,mark=at position 0.08 with
		{\arrow[scale=5,>=stealth]{<}}}]
	\centering
	\begin{scope}
	
	% define constants
	\def \w {8}
	\def \d {4}
	\def \textoff {3}
	\def \r {6}
	
	% Draw the grids
	\draw[step=2,black,very thin] (0,0) grid (\w,\w);
	\draw[step=4,black,very thin] (\w+\d,0) grid (2*\w+\d,8);
	\draw[step=8,black,very thin] (2*\w+2*\d,0) grid (3*\w+2*\d,8);
	
	% First text
	\node[anchor=center,scale=2.5] at (\w+\d/2,\w+\textoff) {Restriction};
	\node[anchor=center,scale=2.5] at (\w+\d/2.1,-\textoff) {Prolongation};
	
	% Second text
	\node[anchor=center,scale=2.5] at (2*\w+1.5*\d,\w+\textoff) {Restriction};
	\node[anchor=center,scale=2.5] at (2*\w+1.5*\d,-\textoff) {Prolongation};
	
	% Top arrows
	\draw[postaction={decorate}] (\w+\d+\r/4,\w+0.5) arc (55:125:\r);
	\draw[postaction={decorate}] (2*\w+2*\d+\r/4,\w+0.5) arc (55:125:\r);
	
	% Bottom arrows
	\draw[postaction={decorate}] (\w-\r/4,-0.5) arc (180+55:180+125:\r);
	\draw[postaction={decorate}] (2*\w+\d-\r/4,-0.5) arc (180+55:180+125:\r);
	
	\end{scope}
	\end{tikzpicture}
	}
	
	\caption{\label{fig:pro_res_ops} Prolongation and restriction operators moving between mesh levels.}
\end{figure}



Two transfer operators are required for each mesh level, except for on the finest and coarsest levels.
Instead, a restriction for the coarsest level and a prolongation the matrix is not needed.
All of these are created in the setup phase of the algorithm.
The algorithm used in the \oomph implementation of operator creation process is described below.

To move from a coarse mesh to a fine mesh:
1. Create the fine mesh
2. For each element in the fine mesh
3. For each node in the element
4. Get the local coordinates of the node
5. Interpolate the value of the solution using the coarse mesh solution value and basis functions
6. End loops

Once the prolongation matrix has been created, the restriction matrix is then defined to be the transpose of the prolongation matrix.
Figure \ref{fig:create_prolong_matrix} shows how solution data for a uniform refinement of a single element is performed.

\begin{figure}[h]
	\centering
	\includegraphics[draft]{images/placeholder}
	\caption{Prolongation matrix creation process. \label{fig:create_prolong_matrix}}
\end{figure}




\subsection{Grid hierarchy}

We will restrict our discussion to uniform structured grids in order to focus on the details of multigrid.
Mathematically, we require only the grid hierarchy so the specific internals of the individual grids is a technical detail that we will omit. 

% We want to discuss how the grid hierarchy is computed and used within oomph-lib.
The grid hierarchy used by multigrid in \oomph is generated by... 



\begin{figure}[h]
	% Show initial refinement, want to show how the hierarchy is created within oomph-lib
	\centering
	\includegraphics[draft]{images/placeholder}
	\caption{Example of a grid hierarchy created for multigrid in \oomph.}
\end{figure}





%------------------------------------------------

\section{Multigrid as a preconditioner}
\label{sec:precond}

\subsection{Block preconditioning}

% Talk about block preconditioning in regards to complex Helmholtz problem
% The block structure, derivation...
% Enumeration of the unknowns and equations. Real part first, imaginary part second

As mentioned in chapter \ref{sec:problem}, the Helmholtz equation we are interested in is complex-valued.
Since \oomph does not natively handle complex arithmetic, complex numbers are instead stored as 2D vectors.
The first component of the vector is the real part of the complex number, and the second component is the imaginary part.

For a specific enumeration of the unknowns, we obtain a matrix whose structure allows us to solve the problem more efficiently.
In our case, we first compute the contribution for all of the real unknowns, then the contribution for the imaginary unknowns.
This gives rise to a Jacobian matrix with block structure
\begin{align}
	A = \begin{pmatrix}
		A_r & -A_c \\ A_c & A_r
	\end{pmatrix}.
\end{align}
Here, $A_r$ is the real part of the discretisation and $A_c$ is the imaginary part.

Given this matrix with a block structure, we will want to use a block preconditioner as our preconditioning matrix.




\subsection{Multigrid preconditioning}

As an alternative to using multigrid as an iterative solver, it is possible to instead perform multigrid iterations to partially solve the linear system, then perform a solve with an additional linear solver on the preconditioned system.
This gives an effective method  

On its own, the linear solver is not as effective as the preconditioned linear solver.


We reduce the tolerance of the mg solver and 





\subsection{FGMRES}

Multigrid preconditioning leads to a variable preconditioner.
That is, between iterations, the preconditioning matrix changes.

Because of this, the standard GMRES method is not a suitable choice of smoother.
Instead, the flexible version of the GMRES algorithm, appropriately named FGMRES, may be used instead.

Only capable of preconditioning on the right


This method was proposed by Saad in \cite{fgmres}.
For a more detailed description of the FGMRES algorithm, readers are directed to this paper.


